{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 201_DataScience_Exercise\n",
    "\n",
    "Recruitment Exercises `oreum_recruit_exercises`\n",
    "\n",
    "2022Q2\n",
    "\n",
    "## Exercise Details\n",
    "\n",
    "### Goals\n",
    "\n",
    "+ Test general capability to learn from data using Python\n",
    "+ Designed to take approx 2 - 3 hours\n",
    "+ Opportunities for extension and non-sequential steps\n",
    "+ This Notebook contains the questions only. See NB202 for the version with worked examples.\n",
    "\n",
    "### Reality check\n",
    "\n",
    "+ This is a brief interview exercise so we can learn how you think and code\n",
    "+ **This is not a thesis or a major project, so please do not spend more than 3 - 4 hours max**\n",
    "+ If you get stuck on a question, just skip over it and we can discuss it later\n",
    "+ We have tried to set fairly straightforward objective tasks, but there's \n",
    "  plenty of room for you to experiment and try various approaches\n",
    "+ Feel free to install more packages if you want, but keep it simple\n",
    "\n",
    "\n",
    "\n",
    "### Contents\n",
    "\n",
    "+ [1. Graphical EDA: Univariate / Multivariate Analysis](#1.-Graphical-EDA:-Univariate-/-Multivariate-Analysis)\n",
    "\n",
    "+ [2. Dimension Reduction / Clustering](#2.-Dimension-Reduction-/-Clustering)\n",
    "\n",
    "+ [3. Classification for Inference using Text Analysis](#3.-Classification-for-Inference-using-Text-Analysis)\n",
    "\n",
    "+ [4. Generalised Linear Regression - Frequency-Severity Decomposition on Expected Delays](#4.-Generalised-Linear-Regression---Frequency-Severity-Decomposition-on-Expected-Delays)\n",
    "\n",
    "\n",
    "## Background on the Data\n",
    "\n",
    "See [100_DataEngineering_ExerciseGuide](100_DataEngineering_ExerciseGuide.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare a default setup for consistency. \n",
    "\n",
    "See `environment.yml` for the packages installed and available in this project. Feel free to modify the file and install more packages if you need them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "sns.set(style='darkgrid', palette='muted', context='notebook', \n",
    "        rc={'savefig.dpi':300, 'figure.figsize': (12, 2)})\n",
    "RSD = 42\n",
    "RNG = np.random.default_rng(seed=RSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Graphical EDA: Univariate / Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks:\n",
    "\n",
    "1. Load the `prepared` table from local DB `data/airlines.sqlite` into a `pandas` dataframe\n",
    "\n",
    "2. Plot the distribution of `statistics_minutes_delayed_total` for all records, confirm that the interquartile range (IQR) over all records is [65k, 165k]\n",
    "\n",
    "3. Plot the distribution of `statistics_minutes_delayed_total`, per `airport_code` - choose a suitable plot type such that we can read off (at least) the means, medians, and IQRs\n",
    "\n",
    "4. Plot the distribution of `statistics_minutes_delayed_total`, per `airport_code`, per `state_code` - focus on the mean, and choose a suitable plot type to deal with this 2D grouping\n",
    "\n",
    "5. Plot the distribution of `proportion_of_flights_on_time` (`statistics_flights_on_time` / `statistics_flights_total`), per `airport_code`, per `survey_date` - choose a suitable plot type to deal with this 2D grouping and bivariate feature\n",
    "\n",
    "6. Plot the proportion of delays per reason (i.e. use the features named `statistics_no_of_delays_*` and `statistics_flights_delayed`), per `survey_date` for `airport_code == 'ATL'` - choose a suitable plot type to deal with this date-based grouping and multivariate set of features\n",
    "\n",
    "\n",
    "### Hints\n",
    "\n",
    "1. Use Python (this project environment contains a baseline useful set of packages including `numpy`, `scipy`, `pandas`, `matplotib`, `seaborn`, and their various dependencies)\n",
    "2. You're free to use the internet! (e.g. [StackOverflow](https://stackoverflow.com))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here, create as many new cells as you like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dimension Reduction / Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let's visualise patterns in the data via clustering / semi-supervised learning:\n",
    "\n",
    "1. Assign a cluster label to the records based on the basic count data:\n",
    "    `['statistics_flights_cancelled', 'statistics_flights_delayed', 'statistics_flights_diverted','statistics_flights_on_time']` \n",
    "    + Check these features sum to `'statistics_flights_total'`\n",
    "    + Consider normalising feature values `(x - mean) / std` \n",
    "    + Use an appropriate clustering technique (many are available in `sklearn`)\n",
    "    \n",
    "    <br/>\n",
    "\n",
    "2. Plot these 5-dimensional clusters on a 2-dimensional scatter plot:\n",
    "    + Consider the manifold tools available in `sklearn`\n",
    "    + Ensure the plot is clear and well-labelled\n",
    "\n",
    "    <br/>\n",
    "\n",
    "3. Describe the patterns you find:\n",
    "    + What we can learn from this clustering analysis?\n",
    "    + What directions might we want to study next? Does the data support it?\n",
    "\n",
    "    <br/>\n",
    "\n",
    "4. Repeat the exercise for aggregations by `survey_date` or `airport_code`\n",
    "    + What time-based or airport location based patterns emerge? Describe them\n",
    "\n",
    "\n",
    "### Hints\n",
    "\n",
    "1. Use Python (this project environment contains a baseline useful set of packages including `numpy`, `scipy`, `pandas`, `matplotib`, `seaborn`, `scikit-learn` and their various dependencies)\n",
    "2. You're free to use the internet! (e.g. [StackOverflow](https://stackoverflow.com))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here, create as many new cells as you like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creative Data Processing and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigation direction\n",
    "\n",
    "Let's get a little creative with regression for inference:\n",
    "\n",
    "+ Imagine that airports can charge a penalty to carriers when the mean count of delays per carrier per month exceeds 100\n",
    "+ Clearly it's not very fair to charge all the carriers for the sins of a few, so:\n",
    "+ **Can we use this dataset to indicate which carriers corrolate with higher mean delays, and guide a further deep dive investigation?**\n",
    "\n",
    "\n",
    "### Hints\n",
    "\n",
    "1. Use Python (this project environment contains a baseline useful set of packages including `numpy`, `scipy`, `pandas`, `matplotib`, `seaborn`, `scikit-learn` and their various dependencies)\n",
    "2. You're free to use the internet! (e.g. [StackOverflow](https://stackoverflow.com))\n",
    "3. Keep it simple\n",
    "4. The feature `'statistics_carriers_names'` contains the names of the carriers per `airport_code` per `survey_date`\n",
    "5. We don't have the counts of delays per carrier, but we could normalise `'statistics_no_of_delays_carrier' / 'statistics_carriers_total'`\n",
    "   \n",
    "### More Hints:\n",
    "1. There's a many to many mapping between carriers and airports, and (of course) airports have different sizes, so you should carefully consider hierarchies, mappings and normalizations\n",
    "2. You could use text analysis to transform data:, to TF-IDF the carrier tokens in the airport documents (`re`, `sklearn.feature_extraction.text.TfidfVectorizer`)\n",
    "3. You should use a model architecture that outputs feature importances, coefficient values, and/or penalises the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1 Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here, create as many new cells as you like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generalised Linear Regression - Frequency-Severity Decomposition on Expected Delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigation direction\n",
    "\n",
    "Let's try something closer to the insurance domain. \n",
    "\n",
    "+ Generally speaking, in order to price a policy, we need to estimate the Expected Loss $\\mathbb{E}_{loss}$ according to attributes of the policy\n",
    "+ The Expected Loss can be decomposed into Frequency (the incidence of a claim) and Severity (the claim amount in the case there is a claim) as the very simple product $\\mathbb{E}_{loss} = Freq * Sev$ \n",
    "+ There's lots of information out there on this decomposition, but for more detail you could start with [this book](https://www.cambridge.org/core/books/abs/predictive-modeling-applications-in-actuarial-science/frequency-and-severity-models/4A11A9A658AEA9C795A5B674C7FE794B), or [this conference presentation](https://www.casact.org/sites/default/files/presentation/spring_2017_presentations_c-12_yan.pdf), or [this conference presentation](https://insurancedatascience.org/downloads/London2021/Session_1a/Jonathan_Sedar.pdf)\n",
    "\n",
    "Notice that in our dataset we have the ingredients to create a frequency and severity (measured in minutes) of delays:\n",
    "```\n",
    "df['sev_minutes_delayed'] = df['statistics_minutes_delayed_total'] / df['statistics_flights_delayed']\n",
    "df['freq_prop_delays'] = df['statistics_flights_delayed'] / df['statistics_flights_total']\n",
    "```\n",
    "\n",
    "So let's try to estimate the Expected Loss:\n",
    "\n",
    "1. Plot `sev_minutes_delayed` vs `freq_prop_delays` such that we can visualise the joint distribution and the marginal distributions\n",
    "2. Create a new feature `df['flights_per_carrier'] = df['statistics_flights_total'] / df['statistics_carriers_total']` and plot that too\n",
    "3. Create two linear models (freq, sev), or a single coupled linear model based on the very simple linear regression formula `1 + flights_per_carrier`, fit the models (inference-only) and comment on the fitted values of the coefficients\n",
    "4. Revise the same model(s) for prediction via holdout set or cross-validation, fit & predict the models, and comment on the quality of prediction and the inference of the fitted coefs\n",
    "5. Comment on any strength or weaknesses of this approach, and any extensions you'd like to do in a full project\n",
    "\n",
    "\n",
    "### Hints\n",
    "\n",
    "1. Use Python (this project environment contains a baseline useful set of packages including `numpy`, `scipy`, `pandas`, `matplotib`, `seaborn`, `scikit-learn`, `patsy` and their various dependencies)\n",
    "2. You're free to use the internet! (e.g. [StackOverflow](https://stackoverflow.com))\n",
    "3. Keep it simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.1 Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here, create as many new cells as you like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: jonathan.sedar@oreum.io\n",
      "\n",
      "Last updated: 2022-06-21 17:17:44\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.9.12\n",
      "IPython version      : 8.3.0\n",
      "\n",
      "Compiler    : Clang 12.0.0 \n",
      "OS          : Darwin\n",
      "Release     : 21.5.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "numpy     : 1.22.3\n",
      "pandas    : 1.4.2\n",
      "matplotlib: 3.5.1\n",
      "sqlite3   : 2.6.0\n",
      "sys       : 3.9.12 (main, Jun  1 2022, 06:36:29) \n",
      "[Clang 12.0.0 ]\n",
      "sklearn   : 1.0.2\n",
      "seaborn   : 0.11.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"<you@email.com>\" -udtmv -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Oreum OÜ &copy; 2022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('oreum_recruit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "fa9de0766857f3b09a20d8ab1c3538581d0840a325fd0e29c5fc99c1949ce3b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
